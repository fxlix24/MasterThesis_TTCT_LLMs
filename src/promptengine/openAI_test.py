import os
from openai import OpenAI
from dotenv import load_dotenv
# Load variables from .env file
load_dotenv("automation.env")

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
prompt = "1. Doorstop: Keep doors open with a sturdy brick. 2. Bookend: Use bricks to support books on a shelf. 3. Paperweight: Prevent papers from blowing away with a hefty brick. 4. Garden Edging: Create borders around flowerbeds or paths. 5. Stepping Stone: Use bricks as makeshift stepping stones in a garden. 6. Fireplace Decor: Arrange bricks for a rustic look. 7. Planter: Drill holes and use as a small planter. 8. Pet Rock: Paint and decorate bricks as artistic pet rocks. 9. Weigh Down Tarps: Secure tarps over items or vehicles outdoors. 10. Workout Tool: Use for weightlifting or stability exercises. 11. Marker: Indicate property lines or garden sections. 12. BBQ Heat Retention: Line BBQ pits to retain heat. 13. Craft Projects: Use as a base for various crafts and sculptures. 14. Raised Bed: Stack to form the walls of a raised garden bed. 15. Temporary Shelter: Use in camping to weigh down tent corners. 16. Splash Guard: Stack at the end of gutters to prevent soil erosion. 17. Furniture Leveler: Correct uneven furniture or appliances. 18. Clothesline Anchor: Tie a line between two bricks to hold a clothesline in place. 19. Sculpture Base: Serve as a solid base for statues or sculptures. 20. Curb Stop: Prevent vehicles from rolling in driveways. 21. Footrest: Use as a makeshift footrest under a desk. 22. Soundproofing: Stack against walls to absorb sound. 23. Path Lighting: Hold solar lights or lanterns in place along a path. 24. Picnic Table Anchor: Weigh down outdoor tablecloths and items."
response = client.responses.create(
    model="gpt-4.1",
    instructions="Use this Metric to evaluate the ideas given to the alternative use test (AUT) regarding the use of a brick. Look at the answers and give for each scoring category an evaluation: \n Fluency: \n It is the number of relevant responses. In scoring fluency, two important criteria must be met; the stimulus must be identifiable and used. In scoring Verbal fluency, the scorers should be sure that the examinees’ verbal responses are relevant to the tasks given. Fluency is the “gatekeeper” for scoring the other TTCT’s components; in other words, any responses that do not meet the criteria for fluency are not scored for anything else. \n Flexibility: \n It is the number of different categories or shifts of ideas. Scorers categorize responses using the list of categories provided in the Verbal Manual for Scoring and Interpreting Results. Examinees gain one point for each new or different category. (In this case use underlying purpose) \n Originality: \n It is the number of unusual ideas as determined by statistical infrequency, implying that rare ideas are original. The scoring guide includes a list of common responses. If the response is not on the list, it is given a point for originality. \n Elaboration:\n It is the addition of ideas beyond the minimum necessary for the response. To determine what elaboration is and is not, it is always good to think of the basics. For example, the basics of a door are the door and the knob. Any additional elements such as the door design or dimensions are considered as elaborated detail. Examinees receive 1 point if the additional ideas range between 0 and 5, and 2 points if the additional ideas are between 6 and 12 ideas. \n Only respond by giving the score as JSON -  no need for an elaborate explanation!: \n \n",
    input=prompt,
    # You can add additional parameters here (e.g., temperature, max_tokens) if supported.
)
print(response.output_text.strip() if hasattr(response, "output_text") else None)
print(getattr(response.usage, "output_tokens", None))
